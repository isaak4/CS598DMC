{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, Softmax\n",
    "from tensorflow.keras.layers import Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz_url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\"\n",
    "glove_url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment lines below to download the files\n",
    "# Amazon file is ~500 MB\n",
    "# Glove file is ~2.1 GB\n",
    "\n",
    "# amz_file = download_file(amz_url)\n",
    "# glove_filename = download_file(glove_url)\n",
    "# amz_file = 'reviews_Electronics_5.json.gz'\n",
    "amz_filename = 'Electronics_5.json'\n",
    "# with gzip.open(amz_file, 'rb') as f_in:\n",
    "#     with open(amz_filename, 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "glove_filename = 'glove.840B.300d.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NUM_WORDS = 5000\n",
    "EMBEDDING_DIM = 300\n",
    "# Validation split is taken care with a parameter validation_split in model.fit()\n",
    "# VALIDATION_SPLIT = 0.25\n",
    "TEST_SPLIT = 0.25\n",
    "\n",
    "# Number of neurons\n",
    "num_lstm = 100\n",
    "num_dense = 200\n",
    "\n",
    "lstm_dropout_rate = 0.2\n",
    "dense_dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing word vectors\n",
      "Indexed the word vectors\n",
      "Found 2196016 word vectors.\n",
      "Wall time: 9.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "########################################\n",
    "## index word vectors.\n",
    "########################################\n",
    "if os.path.isfile('./embedding_index.pickle'):\n",
    "    print('Importing word vectors')\n",
    "    with open('embedding_index.pickle', 'rb') as handle:\n",
    "        embedding_index = pickle.load(handle)\n",
    "else:\n",
    "    print('Indexing word vectors')\n",
    "\n",
    "    encoding = 'utf-8'\n",
    "    embedding_index = {}\n",
    "\n",
    "    with zipfile.ZipFile(glove_filename) as myzip:\n",
    "        with myzip.open(myzip.namelist()[0]) as file:\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "                word = values[0].decode(encoding)\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                embedding_index[word] = vector    \n",
    "    with open('embedding_index.pickle', 'wb') as handle:\n",
    "        pickle.dump(embedding_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('Indexed the word vectors')   \n",
    "print('Found %s word vectors.' %len(embedding_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "amz_reviews_pos = []\n",
    "amz_reviews_neg = []\n",
    "with open(amz_filename, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        rating = int(data['overall'])\n",
    "        if rating != 3:\n",
    "            if (rating == 1) or (rating==2):\n",
    "                amz_reviews_neg.append(data['reviewText'])\n",
    "            else:\n",
    "                amz_reviews_pos.append(data['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 5000 positive and 5000 negative samples.\n",
    "# That corresponds to the 25% of the whole dataset (40000 samples).\n",
    "NUM_POS = 5000\n",
    "NUM_NEG = 5000\n",
    "amz_reviews_neg = random.sample(amz_reviews_neg, NUM_NEG)\n",
    "amz_reviews_pos = random.sample(amz_reviews_pos, NUM_POS)\n",
    "labels = np.asarray([0] * NUM_NEG + [1] * NUM_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79164 unique tokens.\n",
      "Shape of data tensor: (10000, 200)\n",
      "Shape of label tensor: (10000,)\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(amz_reviews_neg + amz_reviews_pos)\n",
    "sequences = tokenizer.texts_to_sequences(amz_reviews_neg + amz_reviews_pos)\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(f'Found {len(tokenizer.word_index)} unique tokens.')\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (7500, 200)\n",
      "Test data shape: (2500, 200)\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set, a validation set and a test set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_test_samples]\n",
    "y_train = labels[:-num_test_samples]\n",
    "x_test = data[-num_test_samples:]\n",
    "y_test = labels[-num_test_samples:]\n",
    "# x_train = data[:-num_validation_samples-num_test_samples]\n",
    "# y_train = labels[:-num_validation_samples-num_test_samples]\n",
    "# x_val = data[-num_validation_samples-num_test_samples:-num_test_samples]\n",
    "# y_val = labels[-num_validation_samples-num_test_samples:-num_test_samples]\n",
    "# x_test = data[-num_test_samples:]\n",
    "# y_test = labels[-num_test_samples:]\n",
    "\n",
    "print('Train data shape:', x_train.shape)\n",
    "# print('Validation data shape:', x_val.shape)\n",
    "print('Test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prepare word embeddings\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word) \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a custome attention layer found in a seq2seq example.\n",
    "class attention(tf.keras.Model):\n",
    "    def __init__(self, units, name):\n",
    "        super().__init__(name=name)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    " \n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    " \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 200, 300)     1500000     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_f (Bidirectional)       [(None, 200, 200), ( 320800      embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_b (Bidirectional)       [(None, 200, 200), ( 240800      bi_lstm_f[0][0]                  \n",
      "                                                                 bi_lstm_f[0][1]                  \n",
      "                                                                 bi_lstm_f[0][2]                  \n",
      "                                                                 bi_lstm_f[0][3]                  \n",
      "                                                                 bi_lstm_f[0][4]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 200)          0           bi_lstm_b[0][1]                  \n",
      "                                                                 bi_lstm_b[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (attention)     ((None, 200), (None, 80601       bi_lstm_b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (Dense)             (None, 1)            201         attention_layer[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 2,142,402\n",
      "Trainable params: 642,402\n",
      "Non-trainable params: 1,500,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "input_text = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype = 'int32', name='input')\n",
    "\n",
    "embedding_layer = Embedding(num_words, \n",
    "                            EMBEDDING_DIM, \n",
    "                            embeddings_initializer=Constant(embedding_matrix), \n",
    "                            input_length = MAX_SEQUENCE_LENGTH, \n",
    "                            trainable = False, \n",
    "                            name='embedding_layer')\n",
    "\n",
    "embedded_sequence = embedding_layer(input_text)\n",
    "\n",
    "lstm = keras.layers.Bidirectional(keras.layers.LSTM\n",
    "                                  (num_lstm,\n",
    "                                   dropout=0.2,\n",
    "                                   recurrent_dropout=0.2,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform'), \n",
    "                                   name=\"bi_lstm_f\")(embedded_sequence)\n",
    "\n",
    "lstm, forward_h, forward_c, backward_h, backward_c = \\\n",
    "       keras.layers.Bidirectional(keras.layers.LSTM\n",
    "                                   (num_lstm,\n",
    "                                    dropout=0.2,\n",
    "                                    recurrent_dropout=0.2,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform'),\n",
    "                                    name=\"bi_lstm_b\")(lstm)\n",
    "\n",
    "state_h = Concatenate(name=\"concatenate\")([forward_h, backward_h])\n",
    "# state_c = Concatenate()([forward_c, backward_c])\n",
    "context_vector, attention_weights = attention(200, name=\"attention_layer\")(lstm, state_h)\n",
    "output = Dense(1, activation='sigmoid', name=\"dense_layer\")(context_vector)\n",
    " \n",
    "model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "# print(model.summary())\n",
    "\n",
    "# #########################\n",
    "# ## train the model.\n",
    "# #########################\n",
    "# # model = Model(inputs = [input_text], outputs = preds)\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 157s 5s/step - loss: 0.6496 - accuracy: 0.6204 - val_loss: 0.5742 - val_accuracy: 0.6987\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 148s 5s/step - loss: 0.5213 - accuracy: 0.7371 - val_loss: 0.5773 - val_accuracy: 0.7157\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 148s 5s/step - loss: 0.4727 - accuracy: 0.7710 - val_loss: 0.5279 - val_accuracy: 0.7301\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 145s 5s/step - loss: 0.4558 - accuracy: 0.7833 - val_loss: 0.4776 - val_accuracy: 0.7691\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 150s 5s/step - loss: 0.4310 - accuracy: 0.7954 - val_loss: 0.4930 - val_accuracy: 0.7632\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 147s 5s/step - loss: 0.3867 - accuracy: 0.8244 - val_loss: 0.4271 - val_accuracy: 0.8005\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 144s 5s/step - loss: 0.3660 - accuracy: 0.8396 - val_loss: 0.4495 - val_accuracy: 0.7861\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 146s 5s/step - loss: 0.3596 - accuracy: 0.8402 - val_loss: 0.4164 - val_accuracy: 0.8101\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 145s 5s/step - loss: 0.3127 - accuracy: 0.8617 - val_loss: 0.5186 - val_accuracy: 0.7824\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 147s 5s/step - loss: 0.3511 - accuracy: 0.8437 - val_loss: 0.3952 - val_accuracy: 0.8181\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0,\n",
    "                               patience=2,\n",
    "                               verbose=0, \n",
    "                               mode='auto')\n",
    "\n",
    "# model_checkpoint = ModelCheckpoint(best_model_path, save_best_only = True, save_weights_only = True)\n",
    "\n",
    "history = model.fit(x = x_train,\n",
    "                    y = y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=200,\n",
    "                    validation_split=.25, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# y_test_predicted = model.predict([x_test], batch_size = 1024, verbose = 1)\n",
    "# sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "# sample_submission[classes_to_predict] = y_test_predicted\n",
    "\n",
    "# sample_submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
